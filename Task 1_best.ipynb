{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "640bb7f5-5a77-485e-83df-fd1480e59118",
   "metadata": {
    "papermill": {
     "duration": 0.035078,
     "end_time": "2025-04-04T12:29:37.807639",
     "exception": false,
     "start_time": "2025-04-04T12:29:37.772561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Changelog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f6f10-b8a5-491c-8064-e5cf8d90123f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T12:29:37.821036Z",
     "iopub.status.busy": "2025-04-04T12:29:37.820664Z",
     "iopub.status.idle": "2025-04-04T12:29:37.828810Z",
     "shell.execute_reply": "2025-04-04T12:29:37.828446Z"
    },
    "papermill": {
     "duration": 0.013396,
     "end_time": "2025-04-04T12:29:37.829546",
     "exception": false,
     "start_time": "2025-04-04T12:29:37.816150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# working_directory = os.getcwd() + \"/image_image_translation\"\n",
    "# print(working_directory)\n",
    "# os.listdir(working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf3a2a-73c1-4717-bb2f-8532f20b65b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T12:29:37.837979Z",
     "iopub.status.busy": "2025-04-04T12:29:37.837589Z",
     "iopub.status.idle": "2025-04-04T12:29:42.573022Z",
     "shell.execute_reply": "2025-04-04T12:29:42.572445Z"
    },
    "papermill": {
     "duration": 4.740838,
     "end_time": "2025-04-04T12:29:42.574393",
     "exception": false,
     "start_time": "2025-04-04T12:29:37.833555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch_fidelity\n",
    "!pip install opencv-python\n",
    "!pip install matplotlib\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35e7d19-959b-4e9c-ac81-a9a5140c1b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T12:29:42.583848Z",
     "iopub.status.busy": "2025-04-04T12:29:42.583521Z",
     "iopub.status.idle": "2025-04-04T12:29:46.157233Z",
     "shell.execute_reply": "2025-04-04T12:29:46.156741Z"
    },
    "papermill": {
     "duration": 3.579331,
     "end_time": "2025-04-04T12:29:46.158271",
     "exception": false,
     "start_time": "2025-04-04T12:29:42.578940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch_fidelity\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884c395-d495-4a1a-b7dc-14b3bee6ca7b",
   "metadata": {
    "papermill": {
     "duration": 0.004129,
     "end_time": "2025-04-04T12:29:46.167441",
     "exception": false,
     "start_time": "2025-04-04T12:29:46.163312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e027852-6d5c-4833-947f-a33a091ee9d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T12:29:46.177326Z",
     "iopub.status.busy": "2025-04-04T12:29:46.176991Z",
     "iopub.status.idle": "2025-04-04T12:29:46.197835Z",
     "shell.execute_reply": "2025-04-04T12:29:46.197443Z"
    },
    "papermill": {
     "duration": 0.027037,
     "end_time": "2025-04-04T12:29:46.198535",
     "exception": false,
     "start_time": "2025-04-04T12:29:46.171498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Step 1. Define Generator with Edge Awareness\n",
    "\"\"\"\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "        \n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Memory-efficient self attention module for Generator enhancement.\"\"\"\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        # Reduce channel dimensions more aggressively to save memory\n",
    "        self.channel_reduction = 16  # More aggressive reduction factor (was 8)\n",
    "        \n",
    "        self.query_conv = nn.Conv2d(in_dim, in_dim // self.channel_reduction, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_dim, in_dim // self.channel_reduction, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))  # learnable weight\n",
    "        \n",
    "        # Use scaled dot-product attention for better numerical stability\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([in_dim // self.channel_reduction]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, C, width, height = x.size()\n",
    "        self.scale = self.scale.to(x.device)\n",
    "        \n",
    "        # Project queries, keys, and values\n",
    "        query = self.query_conv(x).view(batch_size, -1, width * height).permute(0, 2, 1)  # B x (W*H) x C'\n",
    "        key = self.key_conv(x).view(batch_size, -1, width * height)  # B x C' x (W*H)\n",
    "        value = self.value_conv(x).view(batch_size, -1, width * height)  # B x C x (W*H)\n",
    "        \n",
    "        # Calculate attention map with scaling for stability\n",
    "        attention = torch.bmm(query, key) / self.scale  # B x (W*H) x (W*H)\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.bmm(value, attention.permute(0, 2, 1))  # B x C x (W*H)\n",
    "        out = out.view(batch_size, C, width, height)  # B x C x W x H\n",
    "        \n",
    "        # Apply learnable weight and residual connection\n",
    "        return self.gamma * out + x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels, edge_aware=True, num_residual_blocks=7, use_attention=True):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Edge awareness - if True, we'll process with internal edge detection\n",
    "        self.edge_aware = edge_aware\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        # Initial convolution block\n",
    "        model_initial = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels, 64, 7),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Downsampling\n",
    "        model_down = []\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(2):\n",
    "            model_down += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "        \n",
    "        # Add attention after downsampling if enabled\n",
    "        # This is where feature maps are at 1/4 size but rich in information\n",
    "        if use_attention:\n",
    "            self.attention1 = SelfAttention(in_features)\n",
    "        \n",
    "        # Residual blocks\n",
    "        model_res = []\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model_res.append(ResidualBlock(in_features))\n",
    "        \n",
    "        # Add attention after residual blocks if enabled\n",
    "        # This allows attention at the bottleneck where transformation is most critical\n",
    "        if use_attention:\n",
    "            self.attention2 = SelfAttention(in_features)\n",
    "        \n",
    "        # Upsampling\n",
    "        model_up = []\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model_up += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "        \n",
    "        # Output layer\n",
    "        model_output = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, in_channels, 7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        \n",
    "        # Store model sections separately\n",
    "        self.model_initial = nn.Sequential(*model_initial)\n",
    "        self.model_down = nn.Sequential(*model_down)\n",
    "        self.model_res = nn.Sequential(*model_res)\n",
    "        self.model_up = nn.Sequential(*model_up)\n",
    "        self.model_output = nn.Sequential(*model_output)\n",
    "    \n",
    "    def extract_edges(self, x):\n",
    "        # Your existing edge detection code (unchanged)\n",
    "        x_gray = torch.mean(x, dim=1, keepdim=True)\n",
    "        \n",
    "        # Apply simple edge detection using Sobel filters\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).to(x.device)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).to(x.device)\n",
    "        \n",
    "        sobel_x = sobel_x.view(1, 1, 3, 3).repeat(1, 1, 1, 1)\n",
    "        sobel_y = sobel_y.view(1, 1, 3, 3).repeat(1, 1, 1, 1)\n",
    "        \n",
    "        # Apply convolution for edge detection\n",
    "        edge_x = nn.functional.conv2d(x_gray, sobel_x, padding=1)\n",
    "        edge_y = nn.functional.conv2d(x_gray, sobel_y, padding=1)\n",
    "        \n",
    "        # Calculate edge magnitude with a small epsilon to prevent numerical issues\n",
    "        edge_mag = torch.sqrt(edge_x ** 2 + edge_y ** 2 + 1e-6)\n",
    "        \n",
    "        # More moderate combination of edge detectors\n",
    "        laplacian = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=torch.float32).to(x.device)\n",
    "        laplacian = laplacian.view(1, 1, 3, 3).repeat(1, 1, 1, 1)\n",
    "        edge_laplacian = torch.abs(nn.functional.conv2d(x_gray, laplacian, padding=1))\n",
    "        \n",
    "        # Combine edge detectors with more balanced weighting (increased Sobel influence)\n",
    "        edge_mag = 0.85 * edge_mag + 0.15 * edge_laplacian\n",
    "        \n",
    "        # Safe batch-wise normalization to [0, 1]\n",
    "        batch_max = torch.max(edge_mag.view(edge_mag.size(0), -1), dim=1, keepdim=True)[0].view(edge_mag.size(0), 1, 1, 1)\n",
    "        edge_mag = edge_mag / (batch_max + 1e-6)\n",
    "        \n",
    "        # More selective thresholding (higher threshold)\n",
    "        edge_mag = torch.sigmoid((edge_mag - 0.2) * 10) * edge_mag\n",
    "        \n",
    "        return edge_mag\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial processing\n",
    "        x = self.model_initial(x)\n",
    "        \n",
    "        # Downsampling\n",
    "        x = self.model_down(x)\n",
    "        \n",
    "        # Apply first attention after downsampling if enabled\n",
    "        if self.use_attention:\n",
    "            x = self.attention1(x)\n",
    "        \n",
    "        # Residual blocks\n",
    "        x = self.model_res(x)\n",
    "        \n",
    "        # Apply second attention after residual blocks if enabled\n",
    "        if self.use_attention:\n",
    "            x = self.attention2(x)\n",
    "        \n",
    "        # Upsampling\n",
    "        x = self.model_up(x)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.model_output(x)\n",
    "        \n",
    "        # Apply edge enhancement if edge-aware\n",
    "        if self.edge_aware:\n",
    "            edges = self.extract_edges(x)\n",
    "            edge_contribution = 0.15 * edges\n",
    "            output = torch.clamp(output + edge_contribution, -1, 1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\"\"\"\n",
    "Step 2. Define Discriminator\n",
    "\"\"\"\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Scale factor for output size calculation\n",
    "        self.scale_factor = 16\n",
    "        \n",
    "        # Use spectral normalization for all conv layers to stabilize training\n",
    "        model = [\n",
    "            # Initial layer - no instance norm\n",
    "            spectral_norm(nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Increasing depth, reducing spatial dimensions\n",
    "            spectral_norm(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            spectral_norm(nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Final layer - output is not normalized\n",
    "            spectral_norm(nn.Conv2d(512, 1, kernel_size=4, padding=1))\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "389e0e44-9d6f-47e2-aa11-2344c78572fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T12:29:46.208109Z",
     "iopub.status.busy": "2025-04-04T12:29:46.207929Z",
     "iopub.status.idle": "2025-04-04T12:29:46.211097Z",
     "shell.execute_reply": "2025-04-04T12:29:46.210752Z"
    },
    "papermill": {
     "duration": 0.008798,
     "end_time": "2025-04-04T12:29:46.211826",
     "exception": false,
     "start_time": "2025-04-04T12:29:46.203028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3. Define Loss\n",
    "\"\"\"\n",
    "# Binary Cross Entropy loss for GAN with reduction mode to help with stability\n",
    "criterion_GAN = nn.MSELoss(reduction='mean')  # Using MSE loss for stability\n",
    "# L1 loss for cycle consistency with reduction mode\n",
    "criterion_cycle = nn.L1Loss(reduction='mean')\n",
    "# L1 loss for identity preservation with reduction mode\n",
    "criterion_identity = nn.L1Loss(reduction='mean')\n",
    "\n",
    "\n",
    "# Helper function to detect and handle NaN/Inf values in gradients\n",
    "def clip_gradients(parameters, max_norm=1.0):\n",
    "    \"\"\"Clips gradients to prevent exploding gradients\"\"\"\n",
    "    torch.nn.utils.clip_grad_norm_(parameters, max_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb5b8d-9e62-42da-8521-eeb917292cb2",
   "metadata": {
    "papermill": {
     "duration": 0.004047,
     "end_time": "2025-04-04T12:29:46.220289",
     "exception": false,
     "start_time": "2025-04-04T12:29:46.216242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initalisation and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa547270-a616-4ac2-83df-7cab20fd2acf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T12:29:46.229844Z",
     "iopub.status.busy": "2025-04-04T12:29:46.229580Z",
     "iopub.status.idle": "2025-04-04T12:29:46.902212Z",
     "shell.execute_reply": "2025-04-04T12:29:46.901744Z"
    },
    "papermill": {
     "duration": 0.678214,
     "end_time": "2025-04-04T12:29:46.903178",
     "exception": false,
     "start_time": "2025-04-04T12:29:46.224964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in CycleGAN model: 25.01 million\n",
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 4. Initialize G and D\n",
    "\"\"\"\n",
    "# Initialize generators with attention mechanism\n",
    "G_AB = Generator(3, edge_aware=True, num_residual_blocks=8, use_attention=True)  # Real to Comic with dual attention\n",
    "G_BA = Generator(3, edge_aware=False, num_residual_blocks=8, use_attention=True)  # Comic to Real with dual attention\n",
    "\n",
    "\n",
    "# Initialize discriminators\n",
    "D_A = Discriminator(3)  # Discriminator for domain A (real faces)\n",
    "D_B = Discriminator(3)  # Discriminator for domain B (comic faces)\n",
    "\n",
    "## Total parameters in CycleGAN should be less than 60MB\n",
    "total_params = sum(p.numel() for p in G_AB.parameters()) + \\\n",
    "               sum(p.numel() for p in G_BA.parameters()) + \\\n",
    "               sum(p.numel() for p in D_A.parameters()) + \\\n",
    "               sum(p.numel() for p in D_B.parameters())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# modification of parameters computation is forbidden\n",
    "\"\"\"\n",
    "total_params_million = total_params / (1024 * 1024)\n",
    "print(f'Total parameters in CycleGAN model: {total_params_million:.2f} million')\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "print(f'cuda: {cuda}')\n",
    "if cuda:\n",
    "    G_AB = G_AB.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "    G_BA = G_BA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "\n",
    "\n",
    "criterion_GAN = criterion_GAN.cuda() if cuda else criterion_GAN\n",
    "criterion_cycle = criterion_cycle.cuda() if cuda else criterion_cycle\n",
    "criterion_identity = criterion_identity.cuda() if cuda else criterion_identity\n",
    "\n",
    "\"\"\"\n",
    "Step 5. Configure Optimizers\n",
    "\"\"\"\n",
    "lr = 0.0001  # Reduced learning rate for more stability (from 0.0002 to 0.0001)\n",
    "# Add weight decay to improve stability and prevent overfitting\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), \n",
    "                              lr=lr, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "\n",
    "# Fine-tuned learning rate scheduler with early decay and cosine annealing\n",
    "def create_warmup_cosine_scheduler(optimizer, n_epochs, warmup_epochs=3):\n",
    "    import math\n",
    "    \n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            # Gradual warmup\n",
    "            return epoch / warmup_epochs\n",
    "        else:\n",
    "            # Cosine annealing with longer initial plateau\n",
    "            progress = (epoch - warmup_epochs) / (n_epochs - warmup_epochs)\n",
    "            # Modified cosine that starts with less decay\n",
    "            return 0.5 * (1 + math.cos(math.pi * progress ** 1.5))\n",
    "    \n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# Create schedulers\n",
    "scheduler_G = create_warmup_cosine_scheduler(optimizer_G, n_epochs=150)\n",
    "scheduler_D_A = create_warmup_cosine_scheduler(optimizer_D_A, n_epochs=150)\n",
    "scheduler_D_B = create_warmup_cosine_scheduler(optimizer_D_B, n_epochs=150)\n",
    "\n",
    "\"\"\"\n",
    "Step 6. DataLoader\n",
    "\"\"\"\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode='train', transforms=None):\n",
    "        A_dir = os.path.join(data_dir, 'VAE_generation/train')  # modification forbidden\n",
    "        B_dir = os.path.join(data_dir, 'VAE_generation_Cartoon/train')  # modification forbidden\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.files_A = [os.path.join(A_dir, name) for name in sorted(os.listdir(A_dir))[:3200]]  # Adjusted to use more data\n",
    "            self.files_B = [os.path.join(B_dir, name) for name in sorted(os.listdir(B_dir))[:3200]]  # Adjusted to use more data\n",
    "        elif mode == 'valid':\n",
    "            self.files_A = [os.path.join(A_dir, name) for name in sorted(os.listdir(A_dir))[3200:4000]]  # Adjusted for validation\n",
    "            self.files_B = [os.path.join(B_dir, name) for name in sorted(os.listdir(B_dir))[3200:4000]]  # Adjusted for validation\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files_A)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_A = self.files_A[index]\n",
    "        file_B = self.files_B[index % len(self.files_B)]  # Ensure we don't go out of bounds\n",
    "\n",
    "        img_A = Image.open(file_A)\n",
    "        img_B = Image.open(file_B)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img_A = self.transforms(img_A)\n",
    "            img_B = self.transforms(img_B)\n",
    "\n",
    "        return img_A, img_B\n",
    "\n",
    "data_dir = '/kaggle/input/image-image-translation/image_image_translation'  # TODO - Update this when submitting (Kaggle)\n",
    "# data_dir = working_directory  # TODO - Update this when submitting (Jupyter)\n",
    "\n",
    "image_size = (256, 256)\n",
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    ImageDataset(data_dir, mode='train', transforms=transforms_),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=3\n",
    ")\n",
    "\n",
    "validloader = DataLoader(\n",
    "    ImageDataset(data_dir, mode='valid', transforms=transforms_),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "151d77b4-caca-462d-9f0e-7317f2267e8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T12:29:46.913317Z",
     "iopub.status.busy": "2025-04-04T12:29:46.913131Z",
     "iopub.status.idle": "2025-04-04T12:29:46.919309Z",
     "shell.execute_reply": "2025-04-04T12:29:46.918953Z"
    },
    "papermill": {
     "duration": 0.011542,
     "end_time": "2025-04-04T12:29:46.920024",
     "exception": false,
     "start_time": "2025-04-04T12:29:46.908482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_images(real_A, real_B):\n",
    "    \"\"\"\n",
    "    Generate and display sample translations with flexible batch size support.\n",
    "    \n",
    "    Args:\n",
    "        real_A: Batch of images from domain A (real faces)\n",
    "        real_B: Batch of images from domain B (comic style)\n",
    "    \"\"\"\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake_B = G_AB(real_A)\n",
    "        fake_A = G_BA(real_B)\n",
    "        recov_A = G_BA(fake_B)\n",
    "        recov_B = G_AB(fake_A)\n",
    "    \n",
    "    # Get actual batch size from input\n",
    "    batch_size = min(real_A.size(0), real_B.size(0))\n",
    "    \n",
    "    # Create a dynamic figure size based on batch size\n",
    "    fig, ax = plt.subplots(4, batch_size, figsize=(3*batch_size, 12))\n",
    "    \n",
    "    # Handle the case where batch_size is 1\n",
    "    if batch_size == 1:\n",
    "        ax = ax.reshape(4, 1)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        ax[0, i].imshow(real_A[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "        ax[0, i].set_title(\"Real A\")\n",
    "        ax[0, i].axis(\"off\")\n",
    "        \n",
    "        ax[1, i].imshow(fake_B[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "        ax[1, i].set_title(\"Fake B\")\n",
    "        ax[1, i].axis(\"off\")\n",
    "        \n",
    "        ax[2, i].imshow(real_B[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "        ax[2, i].set_title(\"Real B\")\n",
    "        ax[2, i].axis(\"off\")\n",
    "        \n",
    "        ax[3, i].imshow(fake_A[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "        ax[3, i].set_title(\"Fake A\")\n",
    "        ax[3, i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    G_AB.train()\n",
    "    G_BA.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7609be7-5f48-4956-aa67-cfc431ef19e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T12:29:46.929852Z",
     "iopub.status.busy": "2025-04-04T12:29:46.929617Z",
     "iopub.status.idle": "2025-04-04T12:29:46.954729Z",
     "shell.execute_reply": "2025-04-04T12:29:46.954172Z"
    },
    "papermill": {
     "duration": 0.031109,
     "end_time": "2025-04-04T12:29:46.955478",
     "exception": false,
     "start_time": "2025-04-04T12:29:46.924369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_and_save_best_model(epoch, G_AB, G_BA, data_dir, save_path='./best_models'):\n",
    "    \"\"\"\n",
    "    Evaluates current model performance using GMS score and saves the best model.\n",
    "    Updated to work with attention-enhanced generators.\n",
    "    \n",
    "    Args:\n",
    "        epoch (int): Current training epoch\n",
    "        G_AB (nn.Module): Generator for Real to Comic transformation\n",
    "        G_BA (nn.Module): Generator for Comic to Real transformation\n",
    "        data_dir (str): Path to the data directory\n",
    "        save_path (str): Directory to save the best model\n",
    "    \n",
    "    Returns:\n",
    "        float: Current average GMS score\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import torch_fidelity\n",
    "    import shutil\n",
    "    from PIL import Image\n",
    "    import torchvision.transforms as transforms\n",
    "    import gc  # For garbage collection to manage memory\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    temp_save_dir_cartoon = f'./temp_generated_cartoon'\n",
    "    temp_save_dir_real = f'./temp_generated_real'\n",
    "    \n",
    "    # Remove and recreate temporary directories\n",
    "    for dir_path in [temp_save_dir_cartoon, temp_save_dir_real]:\n",
    "        if os.path.exists(dir_path):\n",
    "            shutil.rmtree(dir_path)\n",
    "        os.makedirs(dir_path)\n",
    "    \n",
    "    # Setup for evaluation\n",
    "    image_size = (256, 256)\n",
    "    transforms_ = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    to_image = transforms.ToPILImage()\n",
    "    batch_size = 4  # Reduced batch size to accommodate attention mechanism\n",
    "    \n",
    "    # Use CUDA if available\n",
    "    cuda = torch.cuda.is_available()\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    try:\n",
    "        # 1. Raw Image to Cartoon Image (A→B)\n",
    "        test_dir_A = os.path.join(data_dir, 'VAE_generation/test')\n",
    "        files_A = [os.path.join(test_dir_A, name) for name in os.listdir(test_dir_A)]\n",
    "        \n",
    "        # Process files in batches\n",
    "        for i in range(0, len(files_A), batch_size):\n",
    "            # Read images\n",
    "            imgs = []\n",
    "            for j in range(i, min(len(files_A), i+batch_size)):\n",
    "                img = Image.open(files_A[j])\n",
    "                img = transforms_(img)\n",
    "                imgs.append(img)\n",
    "            imgs = torch.stack(imgs, 0).type(Tensor)\n",
    "            \n",
    "            # Generate fake images\n",
    "            with torch.no_grad():\n",
    "                fake_imgs = G_AB(imgs).detach().cpu()\n",
    "            \n",
    "            # Save generated images\n",
    "            for j in range(fake_imgs.size(0)):\n",
    "                img = fake_imgs[j].squeeze().permute(1, 2, 0)\n",
    "                img_arr = img.numpy()\n",
    "                img_arr = (img_arr - np.min(img_arr)) * 255 / (np.max(img_arr) - np.min(img_arr))\n",
    "                img_arr = img_arr.astype(np.uint8)\n",
    "                \n",
    "                img = to_image(img_arr)\n",
    "                _, name = os.path.split(files_A[i+j])\n",
    "                img.save(os.path.join(temp_save_dir_cartoon, name))\n",
    "            \n",
    "            # Clear memory\n",
    "            del imgs, fake_imgs\n",
    "            if cuda:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate metrics for A→B\n",
    "        gt_dir_B = os.path.join(data_dir, 'VAE_generation_Cartoon/test')\n",
    "        \n",
    "        metrics_AB = torch_fidelity.calculate_metrics(\n",
    "            input1=temp_save_dir_cartoon,\n",
    "            input2=gt_dir_B,\n",
    "            cuda=cuda,\n",
    "            fid=True,\n",
    "            isc=True\n",
    "        )\n",
    "        \n",
    "        fid_score_AB = metrics_AB[\"frechet_inception_distance\"]\n",
    "        is_score_AB = metrics_AB[\"inception_score_mean\"]\n",
    "        \n",
    "        if is_score_AB > 0:\n",
    "            gms_AB = np.sqrt(fid_score_AB / is_score_AB)\n",
    "        else:\n",
    "            gms_AB = float('inf')\n",
    "            \n",
    "        # 2. Cartoon Image to Raw Image (B→A)\n",
    "        test_dir_B = os.path.join(data_dir, 'VAE_generation_Cartoon/test')\n",
    "        files_B = [os.path.join(test_dir_B, name) for name in os.listdir(test_dir_B)]\n",
    "        \n",
    "        # Process files in batches\n",
    "        for i in range(0, len(files_B), batch_size):\n",
    "            # Read images\n",
    "            imgs = []\n",
    "            for j in range(i, min(len(files_B), i+batch_size)):\n",
    "                img = Image.open(files_B[j])\n",
    "                img = transforms_(img)\n",
    "                imgs.append(img)\n",
    "            imgs = torch.stack(imgs, 0).type(Tensor)\n",
    "            \n",
    "            # Generate fake images\n",
    "            with torch.no_grad():\n",
    "                fake_imgs = G_BA(imgs).detach().cpu()\n",
    "            \n",
    "            # Save generated images\n",
    "            for j in range(fake_imgs.size(0)):\n",
    "                img = fake_imgs[j].squeeze().permute(1, 2, 0)\n",
    "                img_arr = img.numpy()\n",
    "                img_arr = (img_arr - np.min(img_arr)) * 255 / (np.max(img_arr) - np.min(img_arr))\n",
    "                img_arr = img_arr.astype(np.uint8)\n",
    "                \n",
    "                img = to_image(img_arr)\n",
    "                _, name = os.path.split(files_B[i+j])\n",
    "                img.save(os.path.join(temp_save_dir_real, name))\n",
    "            \n",
    "            # Clear memory\n",
    "            del imgs, fake_imgs\n",
    "            if cuda:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Calculate metrics for B→A\n",
    "        gt_dir_A = os.path.join(data_dir, 'VAE_generation/test')\n",
    "        \n",
    "        metrics_BA = torch_fidelity.calculate_metrics(\n",
    "            input1=temp_save_dir_real,\n",
    "            input2=gt_dir_A,\n",
    "            cuda=cuda,\n",
    "            fid=True,\n",
    "            isc=True\n",
    "        )\n",
    "        \n",
    "        fid_score_BA = metrics_BA[\"frechet_inception_distance\"]\n",
    "        is_score_BA = metrics_BA[\"inception_score_mean\"]\n",
    "        \n",
    "        if is_score_BA > 0:\n",
    "            gms_BA = np.sqrt(fid_score_BA / is_score_BA)\n",
    "        else:\n",
    "            gms_BA = float('inf')\n",
    "        \n",
    "        # Calculate average GMS\n",
    "        avg_gms = np.round((gms_AB + gms_BA) / 2, 5)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nEvaluation at Epoch {epoch}:\")\n",
    "        print(f\"Raw to Cartoon - IS: {is_score_AB}, FID: {fid_score_AB}, GMS: {gms_AB}\")\n",
    "        print(f\"Cartoon to Raw - IS: {is_score_BA}, FID: {fid_score_BA}, GMS: {gms_BA}\")\n",
    "        print(f\"Average GMS: {avg_gms}\")\n",
    "        \n",
    "        # Load best GMS so far\n",
    "        best_gms_file = os.path.join(save_path, 'best_gms.txt')\n",
    "        if os.path.exists(best_gms_file):\n",
    "            with open(best_gms_file, 'r') as f:\n",
    "                best_gms = float(f.read().strip())\n",
    "        else:\n",
    "            best_gms = float('inf')\n",
    "        \n",
    "        # If current model is better, save it\n",
    "        if avg_gms < best_gms:\n",
    "            print(f\"New best model found! Average GMS improved from {best_gms} to {avg_gms}\")\n",
    "            \n",
    "            # Save the model weights\n",
    "            torch.save(G_AB.state_dict(), os.path.join(save_path, 'G_AB_best.pth'))\n",
    "            torch.save(G_BA.state_dict(), os.path.join(save_path, 'G_BA_best.pth'))\n",
    "            \n",
    "            # Save the architecture information for loading\n",
    "            with open(os.path.join(save_path, 'model_config.txt'), 'w') as f:\n",
    "                f.write(f\"G_AB_attention: {G_AB.use_attention if hasattr(G_AB, 'use_attention') else False}\\n\")\n",
    "                f.write(f\"G_BA_attention: {G_BA.use_attention if hasattr(G_BA, 'use_attention') else False}\\n\")\n",
    "                f.write(f\"G_AB_residual_blocks: {G_AB.num_residual_blocks if hasattr(G_AB, 'num_residual_blocks') else 7}\\n\")\n",
    "                f.write(f\"G_BA_residual_blocks: {G_BA.num_residual_blocks if hasattr(G_BA, 'num_residual_blocks') else 6}\\n\")\n",
    "            \n",
    "            # Save the epoch number and metrics\n",
    "            with open(os.path.join(save_path, 'best_epoch.txt'), 'w') as f:\n",
    "                f.write(str(epoch))\n",
    "            \n",
    "            with open(best_gms_file, 'w') as f:\n",
    "                f.write(str(avg_gms))\n",
    "                \n",
    "            # Save detailed metrics\n",
    "            with open(os.path.join(save_path, 'best_metrics.txt'), 'w') as f:\n",
    "                f.write(f\"Epoch: {epoch}\\n\")\n",
    "                f.write(f\"Raw to Cartoon - IS: {is_score_AB}, FID: {fid_score_AB}, GMS: {gms_AB}\\n\")\n",
    "                f.write(f\"Cartoon to Raw - IS: {is_score_BA}, FID: {fid_score_BA}, GMS: {gms_BA}\\n\")\n",
    "                f.write(f\"Average GMS: {avg_gms}\")\n",
    "        else:\n",
    "            print(f\"No improvement. Current GMS: {avg_gms}, Best GMS: {best_gms}\")\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Runtime error during evaluation: {e}\")\n",
    "        print(\"This is likely due to memory issues. Try reducing the batch size further.\")\n",
    "        # Return a high GMS to avoid saving this model\n",
    "        return float('inf')\n",
    "    \n",
    "    finally:\n",
    "        # Reset models back to training mode\n",
    "        G_AB.train()\n",
    "        G_BA.train()\n",
    "        \n",
    "        # Clean up\n",
    "        for dir_path in [temp_save_dir_cartoon, temp_save_dir_real]:\n",
    "            if os.path.exists(dir_path):\n",
    "                shutil.rmtree(dir_path)\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        if cuda:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return avg_gms\n",
    "\n",
    "\n",
    "def load_best_model(G_AB, G_BA, save_path='./best_models'):\n",
    "    \"\"\"\n",
    "    Loads the best model saved during training.\n",
    "    Updated to handle attention-enhanced generators.\n",
    "    \n",
    "    Args:\n",
    "        G_AB (nn.Module): Generator for Real to Comic transformation\n",
    "        G_BA (nn.Module): Generator for Comic to Real transformation\n",
    "        save_path (str): Directory where the best model is saved\n",
    "    \n",
    "    Returns:\n",
    "        tuple: The loaded models (G_AB, G_BA) and the best epoch\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    \n",
    "    # Check if best model exists\n",
    "    g_ab_path = os.path.join(save_path, 'G_AB_best.pth')\n",
    "    g_ba_path = os.path.join(save_path, 'G_BA_best.pth')\n",
    "    epoch_path = os.path.join(save_path, 'best_epoch.txt')\n",
    "    config_path = os.path.join(save_path, 'model_config.txt')\n",
    "    \n",
    "    if not (os.path.exists(g_ab_path) and os.path.exists(g_ba_path)):\n",
    "        print(\"Best model files not found.\")\n",
    "        return G_AB, G_BA, None\n",
    "    \n",
    "    # Check if we need to reinitialize the models with different configurations\n",
    "    if os.path.exists(config_path):\n",
    "        config = {}\n",
    "        with open(config_path, 'r') as f:\n",
    "            for line in f:\n",
    "                key, value = line.strip().split(': ')\n",
    "                if value.lower() == 'true':\n",
    "                    config[key] = True\n",
    "                elif value.lower() == 'false':\n",
    "                    config[key] = False\n",
    "                else:\n",
    "                    try:\n",
    "                        config[key] = int(value)\n",
    "                    except ValueError:\n",
    "                        config[key] = value\n",
    "        \n",
    "        # Check if we need to reinitialize G_AB with different attention setting\n",
    "        if hasattr(G_AB, 'use_attention') and G_AB.use_attention != config.get('G_AB_attention', False):\n",
    "            print(f\"Reinitializing G_AB with attention={config.get('G_AB_attention', False)}\")\n",
    "            # Assuming Generator class and required imports are available\n",
    "            G_AB = Generator(\n",
    "                3, \n",
    "                edge_aware=True, \n",
    "                num_residual_blocks=config.get('G_AB_residual_blocks', 7),\n",
    "                use_attention=config.get('G_AB_attention', False)\n",
    "            )\n",
    "        \n",
    "        # Check if we need to reinitialize G_BA with different attention setting\n",
    "        if hasattr(G_BA, 'use_attention') and G_BA.use_attention != config.get('G_BA_attention', False):\n",
    "            print(f\"Reinitializing G_BA with attention={config.get('G_BA_attention', False)}\")\n",
    "            # Assuming Generator class and required imports are available\n",
    "            G_BA = Generator(\n",
    "                3, \n",
    "                edge_aware=False, \n",
    "                num_residual_blocks=config.get('G_BA_residual_blocks', 6),\n",
    "                use_attention=config.get('G_BA_attention', False)\n",
    "            )\n",
    "    \n",
    "    # Load best model weights\n",
    "    G_AB.load_state_dict(torch.load(g_ab_path))\n",
    "    G_BA.load_state_dict(torch.load(g_ba_path))\n",
    "    \n",
    "    # Get best epoch\n",
    "    best_epoch = None\n",
    "    if os.path.exists(epoch_path):\n",
    "        with open(epoch_path, 'r') as f:\n",
    "            best_epoch = int(f.read().strip())\n",
    "    \n",
    "    print(f\"Loaded best model from epoch {best_epoch}\")\n",
    "    \n",
    "    # Display best metrics if available\n",
    "    metrics_path = os.path.join(save_path, 'best_metrics.txt')\n",
    "    if os.path.exists(metrics_path):\n",
    "        with open(metrics_path, 'r') as f:\n",
    "            metrics = f.read()\n",
    "        print(\"Best model metrics:\")\n",
    "        print(metrics)\n",
    "    \n",
    "    return G_AB, G_BA, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e49fb-95fb-4e72-9de8-55f610e23695",
   "metadata": {
    "papermill": {
     "duration": 0.004067,
     "end_time": "2025-04-04T12:29:46.964235",
     "exception": false,
     "start_time": "2025-04-04T12:29:46.960168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35325fe-5cd9-4a9d-9291-e8559d9fb5f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T12:29:46.973414Z",
     "iopub.status.busy": "2025-04-04T12:29:46.973189Z",
     "iopub.status.idle": "2025-04-05T11:11:39.458988Z",
     "shell.execute_reply": "2025-04-05T11:11:39.458463Z"
    },
    "papermill": {
     "duration": 81712.491292,
     "end_time": "2025-04-05T11:11:39.459750",
     "exception": false,
     "start_time": "2025-04-04T12:29:46.968458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 7. Training\n",
    "\"\"\"\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "save_dir = './checkpoints'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Number of epochs to train for\n",
    "n_epochs = 150\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (real_A, real_B) in enumerate(trainloader):\n",
    "        real_A, real_B = real_A.type(Tensor), real_B.type(Tensor)\n",
    "\n",
    "        # Create labels dynamically based on discriminator output size\n",
    "        # Instead of calculating from input size and scale factor\n",
    "        # We'll create the tensors after getting actual discriminator outputs\n",
    "        fake_B = G_AB(real_A)\n",
    "        fake_A = G_BA(real_B)\n",
    "        \n",
    "        # Get discriminator outputs to determine correct shape\n",
    "        real_A_out = D_A(real_A)\n",
    "        real_B_out = D_B(real_B)\n",
    "        fake_A_out = D_A(fake_A.detach())\n",
    "        fake_B_out = D_B(fake_B.detach())\n",
    "        \n",
    "        # Create target tensors with correct shapes\n",
    "        valid_A = torch.ones_like(real_A_out).type(Tensor)\n",
    "        fake_A_target = torch.zeros_like(fake_A_out).type(Tensor)\n",
    "        valid_B = torch.ones_like(real_B_out).type(Tensor)\n",
    "        fake_B_target = torch.zeros_like(fake_B_out).type(Tensor)\n",
    "\n",
    "        \"\"\"Train Generators\"\"\"\n",
    "        # Set to training mode \n",
    "        G_AB.train()\n",
    "        G_BA.train()\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Note: fake_A and fake_B already generated earlier to determine shapes\n",
    "        \n",
    "        # Identity loss - helps preserve color composition\n",
    "        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
    "        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
    "        loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "        # GAN loss - train G to make D think the generated images are real\n",
    "        # Use the valid tensors with correct shapes\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid_B)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid_A)\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "        # Cycle loss - ensure we can reconstruct the original image\n",
    "        recov_A = G_BA(fake_B)\n",
    "        recov_B = G_AB(fake_A)\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total loss for generators with more balanced weights\n",
    "        if real_A.size(0) > 0 and real_B.size(0) > 0:  # Ensure we have samples in the batch\n",
    "            # For G_AB (Real→Cartoon): More balanced weights\n",
    "            loss_G_AB = 2.0 * loss_id_B + 1.0 * loss_GAN_AB + 10.0 * loss_cycle_B\n",
    "            \n",
    "            # For G_BA (Cartoon→Real): Keep successful weights\n",
    "            loss_G_BA = 2.0 * loss_id_A + 1.0 * loss_GAN_BA + 10.0 * loss_cycle_A\n",
    "            \n",
    "            # Combined loss\n",
    "            loss_G = (loss_G_AB + loss_G_BA) / 2\n",
    "        else:\n",
    "            # Fallback to combined loss if batch issues\n",
    "            weight1 = 2.0   # Identity weight\n",
    "            weight2 = 1.0   # GAN weight\n",
    "            weight3 = 10.0  # Cycle weight\n",
    "            loss_G = weight1 * loss_identity + weight2 * loss_GAN + weight3 * loss_cycle\n",
    "\n",
    "\n",
    "        # Check for NaN values\n",
    "        if torch.isnan(loss_G):\n",
    "            print(\"NaN detected in generator loss! Skipping backward pass.\")\n",
    "            # Reset the optimizer state\n",
    "            optimizer_G.zero_grad()\n",
    "        else:\n",
    "            loss_G.backward()\n",
    "            # Apply gradient clipping to prevent exploding gradients\n",
    "            clip_gradients(itertools.chain(G_AB.parameters(), G_BA.parameters()), max_norm=1.0)\n",
    "            optimizer_G.step()\n",
    "\n",
    "        \"\"\"Train Discriminator A\"\"\"\n",
    "        optimizer_D_A.zero_grad()\n",
    "\n",
    "        # Use the valid/fake tensors with matching shapes\n",
    "        loss_real = criterion_GAN(real_A_out, valid_A)\n",
    "        loss_fake = criterion_GAN(fake_A_out, fake_A_target)\n",
    "        loss_D_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # Check for NaN values\n",
    "        if torch.isnan(loss_D_A):\n",
    "            print(\"NaN detected in discriminator A loss! Skipping backward pass.\")\n",
    "            # Reset the optimizer state\n",
    "            optimizer_D_A.zero_grad()\n",
    "        else:\n",
    "            loss_D_A.backward()\n",
    "            # Apply gradient clipping\n",
    "            clip_gradients(D_A.parameters(), max_norm=1.0)\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "        \"\"\"Train Discriminator B\"\"\"\n",
    "        optimizer_D_B.zero_grad()\n",
    "\n",
    "        # Use the valid/fake tensors with matching shapes\n",
    "        loss_real = criterion_GAN(real_B_out, valid_B)\n",
    "        loss_fake = criterion_GAN(fake_B_out, fake_B_target)\n",
    "        loss_D_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "        # Check for NaN values\n",
    "        if torch.isnan(loss_D_B):\n",
    "            print(\"NaN detected in discriminator B loss! Skipping backward pass.\")\n",
    "            # Reset the optimizer state\n",
    "            optimizer_D_B.zero_grad()\n",
    "        else:\n",
    "            loss_D_B.backward()\n",
    "            # Apply gradient clipping\n",
    "            clip_gradients(D_B.parameters(), max_norm=1.0)\n",
    "            optimizer_D_B.step()\n",
    "\n",
    "    # Print validation progress\n",
    "    loss_D = (loss_D_A + loss_D_B) / 2\n",
    "    \n",
    "    # Check for NaN values and stop early\n",
    "    if torch.isnan(loss_G) or torch.isnan(loss_D_A) or torch.isnan(loss_D_B):\n",
    "        print(\"NaN detected in loss values. Stopping training early.\")\n",
    "        break\n",
    "        \n",
    "    print(f'[Epoch {epoch + 1}/{n_epochs}]')\n",
    "    print(f'[G loss: {loss_G.item():.4f} | identity: {loss_identity.item():.4f} GAN: {loss_GAN.item():.4f} cycle: {loss_cycle.item():.4f}]')\n",
    "    print(f'[D loss: {loss_D.item():.4f} | D_A: {loss_D_A.item():.4f} D_B: {loss_D_B.item():.4f}]')\n",
    "\n",
    "    # Generate validation samples every 5 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        real_A, real_B = next(iter(validloader))\n",
    "        real_A, real_B = real_A.type(Tensor), real_B.type(Tensor)\n",
    "        sample_images(real_A, real_B)\n",
    "\n",
    "        avg_gms = evaluate_and_save_best_model(\n",
    "            epoch + 1, \n",
    "            G_AB, \n",
    "            G_BA, \n",
    "            data_dir, \n",
    "            save_path='./best_models'\n",
    "        )\n",
    "\n",
    "        print(f'Average GMS for {epoch+1}: {avg_gms}')\n",
    "\n",
    "    # Step the schedulers at the end of each epoch\n",
    "    scheduler_G.step()\n",
    "    scheduler_D_A.step()\n",
    "    scheduler_D_B.step()\n",
    "        \n",
    "\n",
    "G_AB, G_BA, best_epoch = load_best_model(G_AB, G_BA, save_path='./best_models')\n",
    "print(f\"Using best model from epoch {best_epoch} for final evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79668a0b-610a-4eab-8ae1-f2a55d39c9a1",
   "metadata": {
    "papermill": {
     "duration": 1.568958,
     "end_time": "2025-04-05T11:11:42.612220",
     "exception": false,
     "start_time": "2025-04-05T11:11:41.043262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Test Function (Do not Modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3f14c-a7f1-479c-ae9c-177c223582b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T11:11:46.035864Z",
     "iopub.status.busy": "2025-04-05T11:11:46.035652Z",
     "iopub.status.idle": "2025-04-05T11:12:42.960257Z",
     "shell.execute_reply": "2025-04-05T11:12:42.959734Z"
    },
    "papermill": {
     "duration": 60.09037,
     "end_time": "2025-04-05T11:12:44.548674",
     "exception": false,
     "start_time": "2025-04-05T11:11:44.458304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 8. Generate Images\n",
    "\"\"\"\n",
    "## Translation 1: Raw Image --> Cartoon Image\n",
    "test_dir = os.path.join(data_dir, 'VAE_generation/test') # modification forbidden\n",
    "\n",
    "files = [os.path.join(test_dir, name) for name in os.listdir(test_dir)]\n",
    "len(files)\n",
    "\n",
    "save_dir = './Cartoon_images'\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir)  # Deletes the folder and its contents\n",
    "    \n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "to_image = transforms.ToPILImage()\n",
    "\n",
    "G_AB.eval()\n",
    "for i in range(0, len(files), batch_size):\n",
    "    # read images\n",
    "    imgs = []\n",
    "    for j in range(i, min(len(files), i+batch_size)):\n",
    "        img = Image.open(files[j])\n",
    "        img = transforms_(img)\n",
    "        imgs.append(img)\n",
    "    imgs = torch.stack(imgs, 0).type(Tensor)\n",
    "\n",
    "    # generate\n",
    "    fake_imgs = G_AB(imgs).detach().cpu()\n",
    "\n",
    "    # save\n",
    "    for j in range(fake_imgs.size(0)):\n",
    "        img = fake_imgs[j].squeeze().permute(1, 2, 0)\n",
    "        img_arr = img.numpy()\n",
    "        img_arr = (img_arr - np.min(img_arr)) * 255 / (np.max(img_arr) - np.min(img_arr))\n",
    "        img_arr = img_arr.astype(np.uint8)\n",
    "\n",
    "        img = to_image(img_arr)\n",
    "        _, name = os.path.split(files[i+j])\n",
    "        img.save(os.path.join(save_dir, name))\n",
    "\n",
    "gt_dir = os.path.join(data_dir, 'VAE_generation_Cartoon/test')\n",
    "\n",
    "metrics = torch_fidelity.calculate_metrics(\n",
    "    input1=save_dir,\n",
    "    input2=gt_dir,\n",
    "    cuda=True,\n",
    "    fid=True,\n",
    "    isc=True\n",
    ")\n",
    "\n",
    "fid_score = metrics[\"frechet_inception_distance\"]\n",
    "is_score = metrics[\"inception_score_mean\"]\n",
    "\n",
    "if is_score > 0:\n",
    "    s_value_1 = np.sqrt(fid_score / is_score)\n",
    "    print(\"Geometric Mean Score:\", s_value_1)\n",
    "else:\n",
    "    print(\"IS is 0, GMS cannot be computed!\")\n",
    "\n",
    "\n",
    "## Translation 2: Cartoon Image --> Raw Image\n",
    "test_dir = os.path.join(data_dir, 'VAE_generation_Cartoon/test') # modification forbidden\n",
    "\n",
    "files = [os.path.join(test_dir, name) for name in os.listdir(test_dir)]\n",
    "len(files)\n",
    "\n",
    "save_dir = './Raw_images'\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir)  # Deletes the folder and its contents\n",
    "    \n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "G_BA.eval()\n",
    "for i in range(0, len(files), batch_size):\n",
    "    # read images\n",
    "    imgs = []\n",
    "    for j in range(i, min(len(files), i+batch_size)):\n",
    "        img = Image.open(files[j])\n",
    "        img = transforms_(img)\n",
    "        imgs.append(img)\n",
    "    imgs = torch.stack(imgs, 0).type(Tensor)\n",
    "\n",
    "    # generate\n",
    "    fake_imgs = G_BA(imgs).detach().cpu()\n",
    "\n",
    "    # save\n",
    "    for j in range(fake_imgs.size(0)):\n",
    "        img = fake_imgs[j].squeeze().permute(1, 2, 0)\n",
    "        img_arr = img.numpy()\n",
    "        img_arr = (img_arr - np.min(img_arr)) * 255 / (np.max(img_arr) - np.min(img_arr))\n",
    "        img_arr = img_arr.astype(np.uint8)\n",
    "\n",
    "        img = to_image(img_arr)\n",
    "        _, name = os.path.split(files[i+j])\n",
    "        img.save(os.path.join(save_dir, name))\n",
    "\n",
    "gt_dir = os.path.join(data_dir, 'VAE_generation/test')\n",
    "\n",
    "metrics = torch_fidelity.calculate_metrics(\n",
    "    input1 = save_dir,\n",
    "    input2 = gt_dir,\n",
    "    cuda=True,\n",
    "    fid=True,\n",
    "    isc=True\n",
    ")\n",
    "\n",
    "fid_score = metrics[\"frechet_inception_distance\"]\n",
    "is_score = metrics[\"inception_score_mean\"]\n",
    "\n",
    "if is_score > 0:\n",
    "    s_value_2 = np.sqrt(fid_score / is_score)\n",
    "    print(\"Geometric Mean Score:\", s_value_2)\n",
    "else:\n",
    "    print(\"IS is 0, GMS cannot be computed!\")\n",
    "\n",
    "\n",
    "s_value = np.round((s_value_1+s_value_2)/2, 5)\n",
    "df = pd.DataFrame({'id': [1], 'label': [s_value]})\n",
    "\n",
    "print(\"Average GMS:\", s_value)\n",
    "\n",
    "csv_path = \"aaron.kwah.2021.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"CSV saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ed4f3-f96a-4517-adc7-f4d4c3247e06",
   "metadata": {
    "papermill": {
     "duration": 1.583723,
     "end_time": "2025-04-05T11:12:47.765852",
     "exception": false,
     "start_time": "2025-04-05T11:12:46.182129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 81794.54983,
   "end_time": "2025-04-05T11:12:51.086939",
   "environment_variables": {},
   "exception": null,
   "input_path": "/common/home/projectgrps/CS424/CS424G14/CL10-3 150E.ipynb",
   "output_path": "/common/home/projectgrps/CS424/CS424G14/CL10-3 150E.output.04042025202933.ipynb",
   "parameters": {},
   "start_time": "2025-04-04T12:29:36.537109",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
